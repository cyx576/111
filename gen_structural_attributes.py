import os
import re
import h5py
import torch
import numpy as np
from tqdm import tqdm
from transformers import BertModel, BertTokenizer

# ================= é…ç½®åŒº =================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(SCRIPT_DIR, "data", "FB15K-DB15K")
# ç¡®ä¿ä½ å·²ç»ä¸‹è½½äº† models/bert-base-uncased æ–‡ä»¶å¤¹
BERT_PATH = os.path.join(SCRIPT_DIR, "models", "bert-base-uncased")
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"


# ================= ç»“æ„åŒ–å±æ€§åç§°è¯»å–å‡½æ•° =================
def read_attribute_names(filename):
    print(f"Reading attribute names (predicates) from: {filename}")
    attribute_names = set()
    is_dbpedia = 'DB15K' in os.path.basename(filename)
    
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line: continue
            
            attr_name = None
            if not is_dbpedia:  # FB15K style
                # Format: /m/06rf7 \t <http://rdf.freebase.com/ns/location.geocode.longitude> \t 9.70404945
                parts = line.split('\t')
                if len(parts) >= 2:
                    attr_name = parts[1].strip()
            else:  # DB15K style
                # Format: <entity> <predicate> "value"^^<datatype> .
                match = re.search(r'<[^>]+>\s+<(.*?)>', line)
                if match:
                    attr_name = match.group(1).strip()
            
            # ç»Ÿä¸€æ¸…ç†å°–æ‹¬å·ï¼Œç¡®ä¿ä¸ kgs.py å†…éƒ¨é”®æ ¼å¼ä¸€è‡´
            if attr_name and attr_name.startswith('<') and attr_name.endswith('>'):
                attr_name = attr_name[1:-1]
            
            if attr_name:
                attribute_names.add(attr_name)
    
    print(f"Found {len(attribute_names)} unique attribute names.")
    return list(attribute_names)


# ================= BERT ç¼–ç å¹¶ä¿å­˜ H5 æ–‡ä»¶ =================
def generate_embeddings_h5(attr_name_list, output_file, tokenizer, model):
    print(f"Generating embeddings for {output_file}...")
    
    batch_size = 32
    model.eval()
    
    # 1. åˆ›å»º H5 æ–‡ä»¶ï¼Œå‡†å¤‡å­˜å‚¨
    with h5py.File(output_file, 'w') as f:
        dt = h5py.string_dtype(encoding='utf-8')
        
        # å­˜å‚¨å±æ€§åç§°åˆ—è¡¨
        f.create_dataset('attr_name', data=np.array(attr_name_list, dtype=dt))
        
        # å‡†å¤‡å­˜å‚¨åµŒå…¥çš„ dataset (BERT base dim = 768)
        attr_emb_dset = f.create_dataset('attr_emb', (len(attr_name_list), 768), dtype='float32')
        
        # 2. ç¼–ç å¹¶å†™å…¥
        batches = [attr_name_list[i:i + batch_size] for i in range(0, len(attr_name_list), batch_size)]
        
        with torch.no_grad():
            current_index = 0
            for batch in tqdm(batches):
                
                texts = []
                for attr in batch:
                    # ç­–ç•¥ï¼šå°† URI æˆ–è·¯å¾„è½¬æ¢ä¸ºå¯è¯»æ–‡æœ¬ (ä¾‹å¦‚ï¼šhttp://.../areaTotal -> area Total)
                    text = attr.replace('/', ' ').replace('_', ' ').replace('-', ' ').strip()
                    if 'http' in text:
                        match = re.search(r'/([^/]+)$', text)
                        text = match.group(1).replace('_', ' ') if match else text
                    texts.append(text)
                
                # ç¼–ç 
                inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=64).to(DEVICE)
                outputs = model(**inputs)
                embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                
                # å†™å…¥ H5 æ–‡ä»¶
                attr_emb_dset[current_index:current_index + len(embeddings)] = embeddings
                current_index += len(embeddings)
    
    print(f"\nâœ… Saved {len(attr_name_list)} structural attribute embeddings to {output_file}")


# ================= ä¸»æµç¨‹ =================
if __name__ == "__main__":
    if not os.path.exists(os.path.join(BERT_PATH, "pytorch_model.bin")):
        print(
            f"Error: BERT model not found at {BERT_PATH}. Please ensure you have the 'models/bert-base-uncased' folder.")
        exit()
    
    print("Loading BERT model...")
    tokenizer = BertTokenizer.from_pretrained(BERT_PATH)
    model = BertModel.from_pretrained(BERT_PATH).to(DEVICE)
    
    # 2. å¤„ç† FB15K ç»“æ„åŒ–å±æ€§
    fb_attrs = read_attribute_names(os.path.join(DATA_DIR, "FB15K_NumericalTriples.txt"))
    generate_embeddings_h5(fb_attrs, os.path.join(DATA_DIR, "attr_name_1.h5"), tokenizer, model)
    
    # 3. å¤„ç† DB15K ç»“æ„åŒ–å±æ€§
    db_attrs = read_attribute_names(os.path.join(DATA_DIR, "DB15K_NumericalTriples.txt"))
    generate_embeddings_h5(db_attrs, os.path.join(DATA_DIR, "attr_name_2.h5"), tokenizer, model)
    
    print("\nğŸ‰ Structural attribute files (.h5) generated successfully! The training should now run.")